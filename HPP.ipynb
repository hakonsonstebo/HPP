{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Data collection & initial exploration](#Data-collection-&-initial-exploration)\n",
    "  - [Setup and imports](#setup-and-imports)\n",
    "  - [Initial exploration](#initial-exploration)\n",
    "- [Feature engineering](#feature-engineering)\n",
    "  - [Preprocessing](#preprocessing)\n",
    "    - [Data Cleaning](#Data-cleaning)\n",
    "      - [Identify columns with missing values](#identify-columns-with-missing-values)\n",
    "      - [Drop columns](#drop-columns)\n",
    "      - [Impute missing values](#impute-missing-values)\n",
    "  - [Concatenation](#Concatenation)\n",
    "  - [Encoding](#Encoding)\n",
    "  - [Split](#split)\n",
    "- [Model Building](#Model-Building)\n",
    "  - [Model 1 - Linear regression](#Model-1---Linear-regression)\n",
    "    - [First try linear regression & performance](#first-try-linear-regression-&-performance)\n",
    "    - [Enhancement of linear regression](#enhancement-of-linear-regression)\n",
    "  - [Model 2 - Random forest](#Model-2---Random-forest)\n",
    "    - [First try random forest & performance](#first-try-random-forest-&-performance)\n",
    "    - [Enhancement of random forest](#enhancement-of-random-forest)\n",
    "  - [Model 3 - XGBoost](#Model-3---XGBoost)\n",
    "    - [First try XGBoost & performance](#first-try-xgboost-&-performance)\n",
    "    - [Enhancement of XGBoost](#enhancement-of-xgboost)\n",
    "- [Conclusion & submission](#conclusion-&-submission)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection & initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in training data \n",
    "train_df = pd.read_csv('train.csv')\n",
    "#Read in test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "#Delete ID columns\n",
    "train_df.drop(['Id'], axis=1, inplace=True)\n",
    "test_df.drop(['Id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the ID column to make sure this feature does not affect the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show first five rows of dataframe. This gives us an initial overview over what we are dealing with. \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration shows us that the training dataset consists of 1460 rows, and 80 columns. The final column includes the target feature: sale price. The dataset contains various datatypes like int64, float64 and object. We will handle this later under \"encoding\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "categorical = [var for var in train_df.columns if train_df[var].dtype=='O']\n",
    "print('There are {} categorical variables'.format(len(categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "numerical = [var for var in train_df.columns if train_df[var].dtype!='O']\n",
    "print('There are {} numerical variables'.format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  will assume that variables with a definite and low number of unique values are discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the values of the discrete variables\n",
    "discrete = []\n",
    "for var in numerical:\n",
    "    if len(train_df[var].unique())<20:\n",
    "        print(var, ' values: ', train_df[var].unique())\n",
    "        discrete.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we make a list of continuous variables (from the numerical ones)\n",
    "continuous = [var for var in numerical if var not in discrete and var not in ['SalePrice']]\n",
    "continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a correlation matrix to see what is most important for the house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "corrmat = train_df.corr()\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Heatmap\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_df[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "most_corr = pd.DataFrame(cols)\n",
    "most_corr.columns = ['Most Correlated Features']\n",
    "most_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew in SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SalePrice is the variable we need to predict, therefore we want to see how it is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "sns.distplot(train_df['SalePrice'] , fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train_df['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train_df['SalePrice'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is skewed. Thereofre we will use the numpy fuction log1p to see if it help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the numpy fuction log1p\n",
    "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train_df['SalePrice'] , fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train_df['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Plot the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train_df['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 4, figsize=(16, 24))\n",
    "\n",
    "for i, c in enumerate(continuous):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    train_df.plot.scatter(ax=axes[row, col], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df = train_df[train_df['GrLivArea'] < 4500]\n",
    "train_df = train_df[train_df['LotArea'] < 100000]\n",
    "train_df = train_df[train_df['TotalBsmtSF'] < 3000]\n",
    "train_df = train_df[train_df['1stFlrSF'] < 2500]\n",
    "train_df = train_df[train_df['BsmtFinSF1'] < 2000]\n",
    "# train_df = train_df[train_df['LotArea'] < 100000]\n",
    "# train_df = train_df[train_df['LotFrontage'] < 200]\n",
    "# train_df = train_df[train_df['BsmtFinSF1'] < 5000]\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in discrete:\n",
    "    value_counts = train_df[var].value_counts() / len(train_df)\n",
    "    categories_to_remove = value_counts[value_counts < 0.01].index\n",
    "    train_df = train_df[~train_df[var].isin(categories_to_remove)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df['TotalIndoorArea']  = train_test_df['1stFlrSF'] + train_test_df['2ndFlrSF'] + train_test_df['TotalBsmtSF'] + train_test_df['GrLivArea']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting 'SalePrice' vs 'BsmtFinSF1'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(train_test_df['BsmtFinSF1'], train_test_df['SalePrice'], color='blue', label='SalePrice vs BsmtFinSF1')\n",
    "plt.xlabel('BsmtFinSF1')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.title('SalePrice vs BsmtFinSF1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting 'SalePrice' vs 'TotalBsmtSF'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(train_test_df['TotalBsmtSF'], train_test_df['SalePrice'], color='green', label='SalePrice vs TotalBsmtSF')\n",
    "plt.xlabel('TotalBsmtSF')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.title('SalePrice vs TotalBsmtSF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "train_test_df[\"Square_BsmtFinSF1\"] = train_test_df[\"BsmtFinSF1\"] ** 2\n",
    "train_test_df[\"Square_TotalBsmtSF\"] = train_test_df[\"TotalBsmtSF\"] ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Replace NA with None for features where NA means that the house does not have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "            'BsmtFinType1', 'BsmtFinType2',\n",
    "            'GarageType', 'GarageFinish', 'GarageQual', \n",
    "            'GarageCond'):\n",
    "    train_test_df[col] = train_test_df[col].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify columns with missing values\n",
    "The first cleaning of data we are going to do is handle columns with missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of missing data points per column\n",
    "missing_values_count = train_test_df.isnull().sum()\n",
    "#Get columns with at least one missing data point\n",
    "columns_with_missing_data = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "print(columns_with_missing_data)\n",
    "\n",
    "\n",
    "all_data_na = (train_test_df.isnull().sum() / len(train_test_df)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "We will drop columns which are missing more than 80% of their values.\n",
    "<font color='red'>Should find a source which recommends percentage of missingness to drop a column.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of columns which miss more values than 80%.\n",
    "columns_to_drop = missing_values_count[missing_values_count > 0.8*len(train_test_df)].index\n",
    "print(columns_to_drop)\n",
    "train_test_df.drop(columns=columns_to_drop, inplace=True)\n",
    "train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can now see that the four columns \"Alley\", \"PoolQC\", \"Fence\" and \"MiscFeature\" have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values\n",
    "For the rest of the columns with missing values we will impute values. For the numeric columns we will impute the mean of the values in the column, and for the categorical columns we will impute the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df['LotFrontage'] = train_test_df['LotFrontage'].fillna(train_test_df['LotFrontage'].mean())\n",
    "train_test_df['MasVnrArea'] = train_test_df['MasVnrArea'].fillna(train_test_df['MasVnrArea'].mean())\n",
    "train_test_df['GarageYrBlt'] = train_test_df['GarageYrBlt'].fillna(train_test_df['GarageYrBlt'].mean())\n",
    "train_test_df['MasVnrType'] = train_test_df['MasVnrType'].fillna(train_test_df['MasVnrType'].mode()[0])\n",
    "train_test_df['BsmtQual'] = train_test_df['BsmtQual'].fillna(train_test_df['BsmtQual'].mode()[0])\n",
    "train_test_df['BsmtCond'] = train_test_df['BsmtCond'].fillna(train_test_df['BsmtCond'].mode()[0])\n",
    "train_test_df['BsmtExposure'] = train_test_df['BsmtExposure'].fillna(train_test_df['BsmtExposure'].mode()[0])\n",
    "train_test_df['BsmtFinType1'] = train_test_df['BsmtFinType1'].fillna(train_test_df['BsmtFinType1'].mode()[0])\n",
    "train_test_df['BsmtFinType2'] = train_test_df['BsmtFinType2'].fillna(train_test_df['BsmtFinType2'].mode()[0])\n",
    "train_test_df['Electrical'] = train_test_df['Electrical'].fillna(train_test_df['Electrical'].mode()[0])\n",
    "train_test_df['FireplaceQu'] = train_test_df['FireplaceQu'].fillna(train_test_df['FireplaceQu'].mode()[0])\n",
    "train_test_df['GarageType'] = train_test_df['GarageType'].fillna(train_test_df['GarageType'].mode()[0])\n",
    "train_test_df['GarageFinish'] = train_test_df['GarageFinish'].fillna(train_test_df['GarageFinish'].mode()[0])\n",
    "train_test_df['GarageQual'] = train_test_df['GarageQual'].fillna(train_test_df['GarageQual'].mode()[0])\n",
    "train_test_df['GarageCond'] = train_test_df['GarageCond'].fillna(train_test_df['GarageCond'].mode()[0])\n",
    "train_test_df['MSZoning']=train_test_df['MSZoning'].fillna(train_test_df['MSZoning'].mode()[0])\n",
    "train_test_df['Utilities']=train_test_df['Utilities'].fillna(train_test_df['Utilities'].mode()[0])\n",
    "train_test_df['Exterior1st']=train_test_df['Exterior1st'].fillna(train_test_df['Exterior1st'].mode()[0])\n",
    "train_test_df['Exterior2nd']=train_test_df['Exterior2nd'].fillna(train_test_df['Exterior2nd'].mode()[0])\n",
    "train_test_df['MasVnrArea']=train_test_df['MasVnrArea'].fillna(train_test_df['MasVnrArea'].mode()[0])\n",
    "train_test_df['BsmtFinSF1']=train_test_df['BsmtFinSF1'].fillna(train_test_df['BsmtFinSF1'].mean())\n",
    "train_test_df['BsmtFinSF2']=train_test_df['BsmtFinSF2'].fillna(train_test_df['BsmtFinSF2'].mean())\n",
    "train_test_df['BsmtUnfSF']=train_test_df['BsmtUnfSF'].fillna(train_test_df['BsmtUnfSF'].mean())\n",
    "train_test_df['TotalBsmtSF']=train_test_df['TotalBsmtSF'].fillna(train_test_df['TotalBsmtSF'].mean())\n",
    "train_test_df['BsmtFullBath']=train_test_df['BsmtFullBath'].fillna(train_test_df['BsmtFullBath'].mode()[0])\n",
    "train_test_df['BsmtHalfBath']=train_test_df['BsmtHalfBath'].fillna(train_test_df['BsmtHalfBath'].mode()[0])\n",
    "train_test_df['KitchenQual']=train_test_df['KitchenQual'].fillna(train_test_df['KitchenQual'].mode()[0])\n",
    "train_test_df['Functional']=train_test_df['Functional'].fillna(train_test_df['Functional'].mode()[0])\n",
    "train_test_df['GarageYrBlt']=train_test_df['GarageYrBlt'].fillna(train_test_df['GarageYrBlt'].mean())\n",
    "train_test_df['GarageCars']=train_test_df['GarageCars'].fillna(train_test_df['GarageCars'].mean())\n",
    "train_test_df['GarageArea']=train_test_df['GarageArea'].fillna(train_test_df['GarageArea'].mean())\n",
    "train_test_df['SaleType']=train_test_df['SaleType'].fillna(train_test_df['SaleType'].mode()[0])\n",
    "train_test_df['TotalIndoorArea']=train_test_df['TotalIndoorArea'].fillna(train_test_df['TotalIndoorArea'].mean())\n",
    "train_test_df['Square_BsmtFinSF1']=train_test_df['Square_BsmtFinSF1'].fillna(train_test_df['Square_BsmtFinSF1'].mean())\n",
    "train_test_df['Square_TotalBsmtSF']=train_test_df['Square_TotalBsmtSF'].fillna(train_test_df['Square_TotalBsmtSF'].mean())\n",
    "\n",
    "\n",
    "\n",
    "# Check that we have no missing datapoints.\n",
    "missing_values_count = train_test_df.isnull().sum()\n",
    "columns_with_missing_data = missing_values_count[missing_values_count > 0]\n",
    "print(columns_with_missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale price will still miss 1459 values, which are the sale-price of our test set. We ignore that there are missing values here, as it is correct because this is the value we are going to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "To use our models we have to encode the categorical columns in our dataset. To do this we will use the sklearns One hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = train_test_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocess the data to ensure categorical columns contain only strings\n",
    "train_test_df[categorical_columns] = train_test_df[categorical_columns].astype(str)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')\n",
    "\n",
    "# Fit and transform the categorical data using one-hot encoding\n",
    "X_encoded = encoder.fit_transform(train_test_df[categorical_columns])\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = encoder.get_feature_names_out(input_features=categorical_columns)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded features\n",
    "X_encoded_train_test_df = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "\n",
    "# Reset the index of both DataFrames\n",
    "X_encoded_train_test_df.reset_index(drop=True, inplace=True)\n",
    "train_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine the one-hot encoded features with the original numerical features\n",
    "train_test_df = pd.concat([X_encoded_train_test_df, train_test_df.drop(categorical_columns, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "Now we will split the train and test set so we can train a regression model, and then use this model to predict the test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = train_df['SalePrice'].isna()\n",
    "nan_mask = train_test_df['SalePrice'].isna()\n",
    "\n",
    "# Use diff() to find the transition from non-NaN to NaN\n",
    "transition_mask = nan_mask.diff() == True\n",
    "\n",
    "# Find the index of the first occurrence of the transition\n",
    "border_index = transition_mask.idxmax()\n",
    "\n",
    "print(border_index)\n",
    "\n",
    "HPP_data_Train = train_test_df.iloc[:border_index,:]\n",
    "HPP_data_Test = train_test_df.iloc[border_index:,:]\n",
    "\n",
    "#Dropping SalePrice in test-data. This is the value we are going to predict.\n",
    "HPP_data_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression & performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "#Initialize the linear regression model\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "#Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(k_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "rmse= np.sqrt(-cross_val_score(linear_regression_model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the liner_regression_model to the training data\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_regression_model.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual Sale Price (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission of LR predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_regression_model.predict(HPP_data_Test)\n",
    "\n",
    "# Apply np.expm1 to the y_pred values\n",
    "y_pred_expm1 = np.expm1(y_pred)\n",
    "\n",
    "pred_LR=pd.DataFrame(y_pred_expm1)\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_LR['Id'] = range(1461, 1461 + len(pred_LR))\n",
    "pred_LR = pred_LR.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_LR[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Random Forest Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest & performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train should contain your features, and y_train should contain the target variable (SalePrice).\n",
    "X_train = HPP_data_Train.drop(['SalePrice'], axis=1)\n",
    "y_train = HPP_data_Train['SalePrice']\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "#Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(k_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "rmse= np.sqrt(-cross_val_score(random_forest_regressor, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = random_forest_regressor.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual SalePrice (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission of Random Forest prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# Fit the model to the training data\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = random_forest_regressor.predict(HPP_data_Test)\n",
    "# Apply np.expm1 to the y_pred values\n",
    "y_pred_expm1 = np.expm1(y_pred)\n",
    "\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_RF=pd.DataFrame(y_pred_expm1)\n",
    "pred_RF['Id'] = range(1461, 1461 + len(pred_RF))\n",
    "pred_RF = pred_RF.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_RF[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 -  XG boost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X_train should contain your features, and y_train should contain the target variable (SalePrice).\n",
    "X_train = HPP_data_Train.drop(['SalePrice'], axis=1)\n",
    "y_train = HPP_data_Train['SalePrice']\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgboost_regressor = xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=2)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "rmse = np.sqrt(-cross_val_score(xgboost_regressor, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgboost_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgboost_regressor.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual SalePrice (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgboost_regressor = xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=2)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgboost_regressor.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = xgboost_regressor.predict(HPP_data_Test)\n",
    "# Apply np.expm1 to the y_pred values\n",
    "y_pred_expm1 = np.expm1(y_pred)\n",
    "\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_XGB=pd.DataFrame(y_pred_expm1)\n",
    "pred_XGB['Id'] = range(1461, 1461 + len(pred_XGB))\n",
    "pred_XGB = pred_XGB.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_XGB[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best model is PRED_RF\n",
    "pred = pred_XGB\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred['Id'] = range(1461, 1461 + len(pred))\n",
    "pred = pred.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
