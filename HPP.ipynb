{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Data collection & initial exploration](#Data-collection-&-initial-exploration)\n",
    "  - [Setup and imports](#setup-and-imports)\n",
    "  - [Initial exploration](#initial-exploration)\n",
    "- [Feature engineering](#feature-engineering)\n",
    "  - [Preprocessing](#preprocessing)\n",
    "    - [Data Cleaning](#Data-cleaning)\n",
    "      - [Identify columns with missing values](#identify-columns-with-missing-values)\n",
    "      - [Drop columns](#drop-columns)\n",
    "      - [Impute missing values](#impute-missing-values)\n",
    "  - [Concatenation](#Concatenation)\n",
    "  - [Encoding](#Encoding)\n",
    "  - [Split](#split)\n",
    "- [Model Building](#Model-Building)\n",
    "  - [Model 1 - Linear regression](#Model-1---Linear-regression)\n",
    "    - [First try linear regression & performance](#first-try-linear-regression-&-performance)\n",
    "    - [Enhancement of linear regression](#enhancement-of-linear-regression)\n",
    "  - [Model 2 - Random forest](#Model-2---Random-forest)\n",
    "    - [First try random forest & performance](#first-try-random-forest-&-performance)\n",
    "    - [Enhancement of random forest](#enhancement-of-random-forest)\n",
    "  - [Model 3 - XGBoost](#Model-3---XGBoost)\n",
    "    - [First try XGBoost & performance](#first-try-xgboost-&-performance)\n",
    "    - [Enhancement of XGBoost](#enhancement-of-xgboost)\n",
    "- [Conclusion & submission](#conclusion-&-submission)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection & initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in training data \n",
    "train_df = pd.read_csv('train.csv')\n",
    "#Read in test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "#Delete ID columns\n",
    "train_df.drop(['Id'], axis=1, inplace=True)\n",
    "test_df.drop(['Id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the ID column to make sure this feature does not affect the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show first five rows of dataframe. This gives us an initial overview over what we are dealing with. \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploration shows us that the training dataset consists of 1460 rows, and 80 columns. The final column includes the target feature: sale price. The dataset contains various datatypes like int64, float64 and object. We will handle this later under \"encoding\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "categorical = [var for var in train_df.columns if train_df[var].dtype=='O']\n",
    "print('There are {} categorical variables'.format(len(categorical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "numerical = [var for var in train_df.columns if train_df[var].dtype!='O']\n",
    "print('There are {} numerical variables'.format(len(numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  will assume that variables with a definite and low number of unique values are discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the values of the discrete variables\n",
    "discrete = []\n",
    "for var in numerical:\n",
    "    if len(train_df[var].unique())<20:\n",
    "        print(var, ' values: ', train_df[var].unique())\n",
    "        discrete.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we make a list of continuous variables (from the numerical ones)\n",
    "continuous = [var for var in numerical if var not in discrete and var not in ['SalePrice']]\n",
    "continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a correlation matrix to see what is most important for the house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "corrmat = train_df.corr()\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Heatmap\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train_df[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "most_corr = pd.DataFrame(cols)\n",
    "most_corr.columns = ['Most Correlated Features']\n",
    "most_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoxPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplots to visualise outliers in the continuous variables \n",
    "# and histograms to get an idea of the distribution\n",
    "\n",
    "for var in continuous:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fig = sns.boxplot(y=train_df[var])\n",
    "    fig.set_title('')\n",
    "    fig.set_ylabel(var)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    fig = sns.distplot(train_df[var].dropna())\n",
    "    fig.set_ylabel('Number of houses')\n",
    "    fig.set_xlabel(var)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that multiple of the continuous features have outliers. We will handle this under \"Feature engineering\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in continous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qualitative assessment of which features have outliers.\n",
    "outlier_columns = ['1stFlrSF','GrLivArea', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'YearBuilt', 'BsmtFinSF1', \"2ndFlrSF\"]\n",
    "\n",
    "\n",
    "for column in outlier_columns:\n",
    "    # Calculate quartiles\n",
    "    Q1 = train_df[column].quantile(0.25)\n",
    "    Q3 = train_df[column].quantile(0.75)\n",
    "\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define lower and upper bounds\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "    # Filter the DataFrame to exclude outliers\n",
    "    train_df = train_df[(train_df[column] >= lower_bound) & (train_df[column] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers in discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in discrete:\n",
    "    value_counts = train_df[var].value_counts() / len(train_df)\n",
    "    categories_to_remove = value_counts[value_counts < 0.01].index\n",
    "    train_df = train_df[~train_df[var].isin(categories_to_remove)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df['TotalIndoorArea']  = train_test_df['1stFlrSF'] + train_test_df['2ndFlrSF'] + train_test_df['TotalBsmtSF'] + train_test_df['GrLivArea']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plotting 'SalePrice' vs 'BsmtFinSF1'\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(train_test_df['BsmtFinSF1'], train_test_df['SalePrice'], color='blue', label='SalePrice vs BsmtFinSF1')\n",
    "# plt.xlabel('BsmtFinSF1')\n",
    "# plt.ylabel('SalePrice')\n",
    "# plt.title('SalePrice vs BsmtFinSF1')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Plotting 'SalePrice' vs 'TotalBsmtSF'\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(train_test_df['TotalBsmtSF'], train_test_df['SalePrice'], color='green', label='SalePrice vs TotalBsmtSF')\n",
    "plt.xlabel('TotalBsmtSF')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.title('SalePrice vs TotalBsmtSF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "train_test_df[\"Square_BsmtFinSF1\"] = train_test_df[\"BsmtFinSF1\"] ** 2\n",
    "train_test_df[\"Square_TotalBsmtSF\"] = train_test_df[\"TotalBsmtSF\"] ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Replace NA with None for features where NA means that the house does not have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "            'BsmtFinType1', 'BsmtFinType2',\n",
    "            'GarageType', 'GarageFinish', 'GarageQual', \n",
    "            'GarageCond'):\n",
    "    train_test_df[col] = train_test_df[col].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify columns with missing values\n",
    "The first cleaning of data we are going to do is handle columns with missing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of missing data points per column\n",
    "missing_values_count = train_test_df.isnull().sum()\n",
    "#Get columns with at least one missing data point\n",
    "columns_with_missing_data = missing_values_count[missing_values_count > 0]\n",
    "\n",
    "print(columns_with_missing_data)\n",
    "\n",
    "\n",
    "all_data_na = (train_test_df.isnull().sum() / len(train_test_df)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "We will drop columns which are missing more than 80% of their values.\n",
    "<font color='red'>Should find a source which recommends percentage of missingness to drop a column.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of columns which miss more values than 80%.\n",
    "columns_to_drop = missing_values_count[missing_values_count > 0.8*len(train_test_df)].index\n",
    "print(columns_to_drop)\n",
    "train_test_df.drop(columns=columns_to_drop, inplace=True)\n",
    "train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can now see that the four columns \"Alley\", \"PoolQC\", \"Fence\" and \"MiscFeature\" have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values\n",
    "For the rest of the columns with missing values we will impute values. For the numeric columns we will impute the mean of the values in the column, and for the categorical columns we will impute the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df['LotFrontage'] = train_test_df['LotFrontage'].fillna(train_test_df['LotFrontage'].mean())\n",
    "train_test_df['MasVnrArea'] = train_test_df['MasVnrArea'].fillna(train_test_df['MasVnrArea'].mean())\n",
    "train_test_df['GarageYrBlt'] = train_test_df['GarageYrBlt'].fillna(train_test_df['GarageYrBlt'].mean())\n",
    "train_test_df['MasVnrType'] = train_test_df['MasVnrType'].fillna(train_test_df['MasVnrType'].mode()[0])\n",
    "train_test_df['BsmtQual'] = train_test_df['BsmtQual'].fillna(train_test_df['BsmtQual'].mode()[0])\n",
    "train_test_df['BsmtCond'] = train_test_df['BsmtCond'].fillna(train_test_df['BsmtCond'].mode()[0])\n",
    "train_test_df['BsmtExposure'] = train_test_df['BsmtExposure'].fillna(train_test_df['BsmtExposure'].mode()[0])\n",
    "train_test_df['BsmtFinType1'] = train_test_df['BsmtFinType1'].fillna(train_test_df['BsmtFinType1'].mode()[0])\n",
    "train_test_df['BsmtFinType2'] = train_test_df['BsmtFinType2'].fillna(train_test_df['BsmtFinType2'].mode()[0])\n",
    "train_test_df['Electrical'] = train_test_df['Electrical'].fillna(train_test_df['Electrical'].mode()[0])\n",
    "train_test_df['FireplaceQu'] = train_test_df['FireplaceQu'].fillna(train_test_df['FireplaceQu'].mode()[0])\n",
    "train_test_df['GarageType'] = train_test_df['GarageType'].fillna(train_test_df['GarageType'].mode()[0])\n",
    "train_test_df['GarageFinish'] = train_test_df['GarageFinish'].fillna(train_test_df['GarageFinish'].mode()[0])\n",
    "train_test_df['GarageQual'] = train_test_df['GarageQual'].fillna(train_test_df['GarageQual'].mode()[0])\n",
    "train_test_df['GarageCond'] = train_test_df['GarageCond'].fillna(train_test_df['GarageCond'].mode()[0])\n",
    "train_test_df['MSZoning']=train_test_df['MSZoning'].fillna(train_test_df['MSZoning'].mode()[0])\n",
    "train_test_df['Utilities']=train_test_df['Utilities'].fillna(train_test_df['Utilities'].mode()[0])\n",
    "train_test_df['Exterior1st']=train_test_df['Exterior1st'].fillna(train_test_df['Exterior1st'].mode()[0])\n",
    "train_test_df['Exterior2nd']=train_test_df['Exterior2nd'].fillna(train_test_df['Exterior2nd'].mode()[0])\n",
    "train_test_df['MasVnrArea']=train_test_df['MasVnrArea'].fillna(train_test_df['MasVnrArea'].mode()[0])\n",
    "train_test_df['BsmtFinSF1']=train_test_df['BsmtFinSF1'].fillna(train_test_df['BsmtFinSF1'].mean())\n",
    "train_test_df['BsmtFinSF2']=train_test_df['BsmtFinSF2'].fillna(train_test_df['BsmtFinSF2'].mean())\n",
    "train_test_df['BsmtUnfSF']=train_test_df['BsmtUnfSF'].fillna(train_test_df['BsmtUnfSF'].mean())\n",
    "train_test_df['TotalBsmtSF']=train_test_df['TotalBsmtSF'].fillna(train_test_df['TotalBsmtSF'].mean())\n",
    "train_test_df['BsmtFullBath']=train_test_df['BsmtFullBath'].fillna(train_test_df['BsmtFullBath'].mode()[0])\n",
    "train_test_df['BsmtHalfBath']=train_test_df['BsmtHalfBath'].fillna(train_test_df['BsmtHalfBath'].mode()[0])\n",
    "train_test_df['KitchenQual']=train_test_df['KitchenQual'].fillna(train_test_df['KitchenQual'].mode()[0])\n",
    "train_test_df['Functional']=train_test_df['Functional'].fillna(train_test_df['Functional'].mode()[0])\n",
    "train_test_df['GarageYrBlt']=train_test_df['GarageYrBlt'].fillna(train_test_df['GarageYrBlt'].mean())\n",
    "train_test_df['GarageCars']=train_test_df['GarageCars'].fillna(train_test_df['GarageCars'].mean())\n",
    "train_test_df['GarageArea']=train_test_df['GarageArea'].fillna(train_test_df['GarageArea'].mean())\n",
    "train_test_df['SaleType']=train_test_df['SaleType'].fillna(train_test_df['SaleType'].mode()[0])\n",
    "train_test_df['TotalIndoorArea']=train_test_df['TotalIndoorArea'].fillna(train_test_df['TotalIndoorArea'].mean())\n",
    "train_test_df['Square_BsmtFinSF1']=train_test_df['Square_BsmtFinSF1'].fillna(train_test_df['Square_BsmtFinSF1'].mean())\n",
    "train_test_df['Square_TotalBsmtSF']=train_test_df['Square_TotalBsmtSF'].fillna(train_test_df['Square_TotalBsmtSF'].mean())\n",
    "\n",
    "\n",
    "\n",
    "# Check that we have no missing datapoints.\n",
    "missing_values_count = train_test_df.isnull().sum()\n",
    "columns_with_missing_data = missing_values_count[missing_values_count > 0]\n",
    "print(columns_with_missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale price will still miss 1459 values, which are the sale-price of our test set. We ignore that there are missing values here, as it is correct because this is the value we are going to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Skewed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# TODO Haakon look over\n",
    "def find_skewness(train, numeric_cols):\n",
    "    \"\"\"\n",
    "    Calculate the skewness of the columns and segregate the positive\n",
    "    and negative skewed data.\n",
    "    \"\"\"\n",
    "    skew_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        skew_dict[col] = train[col].skew()\n",
    "\n",
    "    skew_dict = dict(sorted(skew_dict.items(),key=itemgetter(1)))\n",
    "    positive_skew_dict = {k:v for (k,v) in skew_dict.items() if v>0}\n",
    "    negative_skew_dict = {k:v for (k,v) in skew_dict.items() if v<0}\n",
    "    return skew_dict, positive_skew_dict, negative_skew_dict\n",
    "\n",
    "def add_constant(data, highly_pos_skewed):\n",
    "    \"\"\"\n",
    "    Look for zeros in the columns. If zeros are present then the log(0) would result in -infinity.\n",
    "    So before transforming it we need to add it with some constant.\n",
    "    \"\"\"\n",
    "    C = 1\n",
    "    for col in highly_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            if(len(data[data[col] == 0]) > 0):\n",
    "                data[col] = data[col] + C\n",
    "    return data\n",
    "\n",
    "def log_transform(data, highly_pos_skewed):\n",
    "    \"\"\"\n",
    "    Log transformation of highly positively skewed columns.\n",
    "    \"\"\"\n",
    "    for col in highly_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            data[col] = np.log10(data[col])\n",
    "    return data\n",
    "\n",
    "def sqrt_transform(data, moderately_pos_skewed):\n",
    "    \"\"\"\n",
    "    Square root transformation of moderately skewed columns.\n",
    "    \"\"\"\n",
    "    for col in moderately_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            data[col] = np.sqrt(data[col])\n",
    "    return data\n",
    "\n",
    "def reflect_sqrt_transform(data, moderately_neg_skewed):\n",
    "    \"\"\"\n",
    "    Reflection and log transformation of highly negatively skewed \n",
    "    columns.\n",
    "    \"\"\"\n",
    "    for col in moderately_neg_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            K = max(data[col]) + 1\n",
    "            data[col] = np.sqrt(K - data[col])\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n",
    "If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n",
    "If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n",
    "\"\"\"\n",
    "skew_dict, positive_skew_dict, negative_skew_dict = find_skewness(train_test_df, numerical)\n",
    "moderately_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>0.5 and v<=1}\n",
    "highly_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>1}\n",
    "moderately_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v>-1 and v<=0.5}\n",
    "highly_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v<-1}\n",
    "\n",
    "'''Transform train data.'''\n",
    "train_test_df  = add_constant(train_test_df , highly_pos_skewed)\n",
    "train_test_df  = log_transform(train_test_df , highly_pos_skewed)\n",
    "train_test_df  = sqrt_transform(train_test_df , moderately_pos_skewed)\n",
    "train_test_df  = reflect_sqrt_transform(train_test_df , moderately_neg_skewed )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "To use our models we have to encode the categorical columns in our dataset. To do this we will use the sklearns One hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = train_test_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocess the data to ensure categorical columns contain only strings\n",
    "train_test_df[categorical_columns] = train_test_df[categorical_columns].astype(str)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')\n",
    "\n",
    "# Fit and transform the categorical data using one-hot encoding\n",
    "X_encoded = encoder.fit_transform(train_test_df[categorical_columns])\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = encoder.get_feature_names_out(input_features=categorical_columns)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded features\n",
    "X_encoded_train_test_df = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "\n",
    "# Reset the index of both DataFrames\n",
    "X_encoded_train_test_df.reset_index(drop=True, inplace=True)\n",
    "train_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Combine the one-hot encoded features with the original numerical features\n",
    "train_test_df = pd.concat([X_encoded_train_test_df, train_test_df.drop(categorical_columns, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "Now we will split the train and test set so we can train a regression model, and then use this model to predict the test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = train_df['SalePrice'].isna()\n",
    "nan_mask = train_test_df['SalePrice'].isna()\n",
    "\n",
    "# Use diff() to find the transition from non-NaN to NaN\n",
    "transition_mask = nan_mask.diff() == True\n",
    "\n",
    "# Find the index of the first occurrence of the transition\n",
    "border_index = transition_mask.idxmax()\n",
    "\n",
    "print(border_index)\n",
    "\n",
    "HPP_data_Train = train_test_df.iloc[:border_index,:]\n",
    "HPP_data_Test = train_test_df.iloc[border_index:,:]\n",
    "\n",
    "#Dropping SalePrice in test-data. This is the value we are going to predict.\n",
    "HPP_data_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression & performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "#Initialize the linear regression model\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "#Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(k_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "rmse= np.sqrt(-cross_val_score(linear_regression_model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "#Below we split the training data into train and test, so that we have labels for the actual Sale price.\n",
    "#This is used to analyze our model to further enhance it. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the liner_regression_model to the training data\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = linear_regression_model.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual Sale Price (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission of LR predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "\n",
    "pred_LR=pd.DataFrame(y_pred)\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_LR['Id'] = range(1461, 1461 + len(pred_LR))\n",
    "pred_LR = pred_LR.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_LR[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_LR.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest & performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train should contain your features, and y_train should contain the target variable (SalePrice).\n",
    "X_train = HPP_data_Train.drop(['SalePrice'], axis=1)\n",
    "y_train = HPP_data_Train['SalePrice']\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=11)\n",
    "\n",
    "#Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(k_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "rmse= np.sqrt(-cross_val_score(random_forest_regressor, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = random_forest_regressor.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual SalePrice (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission of Random Forest prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "pred_RF=pd.DataFrame(y_pred)\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_RF['Id'] = range(1461, 1461 + len(pred_RF))\n",
    "pred_RF = pred_RF.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_RF[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_RF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 -  XG boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X_train should contain your features, and y_train should contain the target variable (SalePrice).\n",
    "X_train = HPP_data_Train.drop(['SalePrice'], axis=1)\n",
    "y_train = HPP_data_Train['SalePrice']\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgboost_regressor = xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=2)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Cross Validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "rmse = np.sqrt(-cross_val_score(xgboost_regressor, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "print(rmse.mean())\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgboost_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = xgboost_regressor.predict(X_test)\n",
    "\n",
    "# Create a scatter plot to visualize the correlation\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, c='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='y = x')\n",
    "plt.xlabel(\"Actual SalePrice (y_test)\")\n",
    "plt.ylabel(\"Predicted Sale Price (y_pred)\")\n",
    "plt.title(\"Correlation between Actual and Predicted Sale Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train contains features, and y_train contains the target variable (SalePrice).\n",
    "X_train= HPP_data_Train.drop(['SalePrice'],axis=1)\n",
    "y_train= HPP_data_Train['SalePrice']\n",
    "pred_XGB=pd.DataFrame(y_pred)\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred_XGB['Id'] = range(1461, 1461 + len(pred_XGB))\n",
    "pred_XGB = pred_XGB.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred_XGB[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our best model is PRED_RF\n",
    "pred = pred_RF\n",
    "# Create a DataFrame with 'Id' values (1461 to N+1460) and the 'SalePrice' values from 'pred'\n",
    "pred['Id'] = range(1461, 1461 + len(pred))\n",
    "pred = pred.rename(columns={0: 'SalePrice'})\n",
    "# Create a new DataFrame with columns named \"Id\" and \"SalePrice\"\n",
    "result_df = pred[['Id', 'SalePrice']]\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('predicted_saleprice.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
